---
title: "PGMs -- An Odyssey"
subtitle: "Digression: Message passing in Phylogenetics"
author: "Ben Teo"
date: "5/9/2022"
output:
  ioslides_presentation:
    self_contained: false
    mathjax: local
    css: styles.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=F)
```

$\newcommand{\indep}{\perp\!\!\!\perp}$
$\newcommand{\QED}{\scriptsize{\square}}$
$\DeclareMathOperator{\argmin}{\arg\min}$
$\DeclareMathOperator{\Var}{var}$
$\DeclareMathOperator{\Cov}{cov}$
$\newcommand{\Ef}{{\mathrm{I}\!\mathrm{E}}}$
$\newcommand{\E}[1]{\Ef\!\left(#1\right)}$

## Multivariate Gaussian models

<font size="4">

Mitov et al. 2020 show how to efficiently calculate likelihoods for the
$\mathcal{G}_{LInv}$ family.

<def>
A trait evolutionary model is in the $\mathcal{G}_{LInv}$ family if the
conditional distribution of the trait vector at time $t$, $\vec{x}(t)$ given
the trait vector at time $s<t$, $\vec{x}(t)$ is Gaussian with
$$\begin{equation}
\begin{split}
\E{\vec{x}(t)\mid\vec{x}(s)} &= \vec{\omega}+\mathbf{\Phi}\vec{x}(s) \\
\Var(\vec{x}(t)\mid\vec{x}(s)) &= \mathbf{V}
\end{split}
\end{equation}$$
</def>
$\vec{x}(t)$ and $\vec{x}(s)$ are not assumed to have the same length, and
$\mathbf{V}\succ 0$.

This is a generalization of the Gaussian BN (i.e. where each CPD is a linear
Gaussian).

</font>

## Multivariate Gaussian models

<font size="4">

<def>
Let $Y$ be a continuous variable with continuous parents $X_1,\ldots,X_k$. $Y$
has a linear Gaussian model if
$$Y\mid\vec{X}\sim\mathcal{N}(\beta_0+\vec{\beta}\cdot\vec{X},\sigma^2)$$
</def>
Gaussian BNs are an alternative representation for the class of multivariate
Gaussian distributions.

For $P\in\mathcal{G}_{LInv}$, each CPD consists of a set of (correlated) linear
Gaussians sharing the same parents.

</font>

## Information form

<font size="4">

If $\vec{X}\sim\mathcal{N}(\vec{\mu},\mathbf{\Sigma})$, then
$$p(\vec{x})\propto\exp(-\frac{1}{2}\vec{x}\cdot \mathbf{J}\vec{x}+\vec{x}\cdot
\mathbf{J}\vec{\mu})$$
where $\mathbf{J}=\mathbf{\Sigma}^{-1}$ is the *information matrix*.

If $p(\vec{x})\propto\exp(-\frac{1}{2}\vec{x}\cdot \mathbf{J}\vec{x}+\vec{x}
\cdot\mathbf{J}\vec{\mu})$, then $\vec{X}\sim\mathcal{N}(\vec{\mu},
\mathbf{J}^{-1})$ iff $\mathbf{J}$ is symmetric and positive definite.

</font>

## Mitov et al. 2020

<font size="4">

**Theorem 1** rewrites each CPD, $p(\vec{x}_i\mid\vec{x}_j)$ as a log-quadratic
polynomial in both $\vec{x}_i$ and $\vec{x}_j$.

- This is slightly more detailed than the information form.

**Theorem 2** says that for any clade in the tree, the conditional density of
its tips given its root can be represented as a log-quadratic polynomial of its
root state $\vec{x}_r$.

- The inductive proof illustrates a linear-time (**in the number of nodes**)
algorithm for computing the marginal probability of the tips conditional on the root.

</font>

## Mitov et al. 2020: Sum-Product VE?

<font size="4">

Generalization of a Gaussian BN. Factors are the CPDs.

Want to compute the marginal of the tips conditioned on the root. Need to sum
out the internal nodes.

At each VE step

  1. We choose an internal node and multiply all factors involving that
  variable. These factors are consumed.
  2. We integrate out that variable from the above product of factors. A new
  factor is created.
  
In their context, the BN is a tree.

  - Each internal node has 1 parent and 1 or more children.
  - We have to multiply the initial factor for the internal node with separate
  intermediate factors generated by each of the internal node's children.

</font>

## Mitov et al. 2020: Clique tree?

<font size="4">

Each cluster corresponds to a CPD (i.e. a parent-child pair in the BN).

  - By construction, this is family-preserving.
  - If the parent in a cluster is the child in another cluster, then link these
  2 clusters by an edge. This satisfies the running-intersection property.

</font>

## Mitov et al. 2020: Canonical form

<font size="4">

Represent the factors (initial \& intermediate) in canonical form,
$\mathcal{C}$.

<def>
Denote $\exp(-\frac{1}{2}\vec{x}\cdot\mathbf{K}\vec{x}+\vec{h}\cdot\vec{x}+g)$
by $\mathcal{C}(\vec{x};\mathbf{K},\vec{h},g)$, where $\mathbf{K}$ is symmetric
without loss of generality.
</def>

  - Factor product is well-defined.
  - The question is if factor marginalization is always well-defined in our
  context.
    - To integrate out $\vec{y}$ from $\mathcal{C}(\vec{y},\vec{x};
    \begin{bmatrix}\mathbf{K}_{yy} & \mathbf{K}_{xy} \\
    \mathbf{K}_{yx} & \mathbf{K}_{xx}\end{bmatrix},\vec{h},g)$, we need
    $\mathbf{K}_{yy}\succ 0$.

</font>

## Mitov et al. 2020: Messages?

<font size="4">

$$\delta_{i\rightarrow j}=\int\phi_i\prod_{k\in\text{Nb}_i\setminus\{j\}}
\delta_{k\rightarrow i}d\vec{x}_i$$
where $\phi_i=p(\vec{x}_i\mid\vec{x}_j)$.

Represent each $\delta_{i\rightarrow j}$ by $\mathcal{C}(\mathbf{K}_i,
\vec{h}_i,g_i)$. The scope of $\delta_{i\rightarrow j}$ is the union of 
$\vec{x}_j$ (this is the parent in clique $\mathbf{C}_j$) with the tips
descended from $\vec{x}_j$, denoted by $\mathbf{X}_j$.

Can we express $(\mathbf{K}_i,\vec{h}_i,g_i)$ in terms of the
$(\mathbf{K}_l,\vec{h}_l,g_l)$s from downstream cliques?

</font>

## Mitov et al. 2020: Solve recursion (I)

<font size="4">

$$\begin{equation}\begin{split}
\delta_{i\rightarrow j} &= \int\phi_i\prod_{k\in\text{Nb}_i\setminus\{j\}}
\mathcal{C}(\mathbf{X}_k,\vec{x}_i;\mathbf{K}_k,\vec{h}_k,g_k)d\vec{x}_i \\
&= \mathcal{C}(\mathbf{X}_i,\vec{x}_j;\mathbf{K}_i,\vec{h}_i,g_i)
\end{split}\end{equation}$$

Since each $\mathcal{C}(\mathbf{X}_k,\vec{x}_i;\mathbf{K}_k,\vec{h}_k,g_k)$ is
log-quadratic in $\vec{x}_i$, we can rewrite them as
$\exp(\vec{x}_i\cdot\mathbf{L}_k\vec{x}_i+\vec{x}_i\cdot\vec{m}_k+r_k)$. This
simplifies our notation when we integrate out $\vec{x}_i$.

$$\begin{equation}\begin{split}
\delta_{i\rightarrow j} &=
\int\phi_i\prod_k\exp(\vec{x}_i\cdot\mathbf{L}_k\vec{x}_i+
\vec{x}_i\cdot\vec{m}_k+r_k)d\vec{x}_i \\
&= \int\phi_i\exp(\vec{x}_i\cdot\sum_k (\mathbf{L}_k\vec{x}_i+\vec{m}_k)+
\sum_k r_k)d\vec{x}_i
\end{split}\end{equation}$$

</font>

## Mitov et al. 2020: Solve recursion (II)

<font size="4">

$$\begin{equation}\begin{split}
\phi_i &= \exp(\vec{x}_i\cdot\mathbf{A}_i\vec{x}_i+\vec{x}_i\cdot\vec{b}_i \\
& \hspace{2em} +\vec{x}_j\cdot\mathbf{C}_i\vec{x}_j+\vec{x}_j\cdot\vec{d}_i \\
& \hspace{2em} +\vec{x}_j\cdot\mathbf{E}_i\vec{x}_i+f_i)
\end{split}\end{equation}$$

$$\begin{equation}\begin{split}
\delta_{i\rightarrow j} &=
\int\exp(\vec{x}_i'(\mathbf{A}_i+\sum_k\mathbf{L}_k)
\vec{x}_i \\
& \hspace{3em} +(\vec{b}_i+\mathbf{E}_i'\vec{x}_j+\sum_k\vec{m}_k)'\vec{x}_i \\
& \hspace{3em} +\sum_k r_k+f_i \\
& \hspace{3em} +\vec{x}_j\cdot\mathbf{C}_i\vec{x}_j+\vec{x}_j\cdot\vec{d}_i)
d\vec{x}_i\end{split}\end{equation}$$

</font>

## Mitov et al. 2020: Solve recursion (III)

<font size="4">

$$\begin{equation}\begin{split}
\delta_{i\rightarrow j} &= \int \exp(\vec{x}_i'\mathbf{A}_i^*\vec{x}_i +
\vec{x}_i'\vec{b}_i^* +\frac{1}{4}\vec{b}_i^*\cdot(\mathbf{A}_i^*)^{-1}
\vec{b}_i^*)d\vec{x}_i\cdot \\
& \hspace{2em} \exp(\sum_k r_k + f_i -\frac{1}{4}\vec{b}_i^*
\cdot(\mathbf{A}_i^*)^{-1}\vec{b}_i^* \\
& \hspace{3em} +\vec{x}_j\cdot\mathbf{C}_i\vec{x}_j+\vec{x}_j\cdot\vec{d}_i) \\
&= \exp(\sum_k r_k + f_i -\frac{1}{4}\vec{b}_i^*
\cdot(\mathbf{A}_i^*)^{-1}\vec{b}_i^*+\vec{x}_j\cdot\mathbf{C}_i\vec{x}_j+
\vec{x}_j\cdot\vec{d}_i \\
& \hspace{2em} +\frac{n_i}{2}\log 2\pi -\frac{1}{2}\log|-2\mathbf{A}_i^*|) \\
&= \exp(\vec{x}_j'(\mathbf{C}_i-\frac{1}{4}\mathbf{E}_i(\mathbf{A}_i^*)^{-1}
\mathbf{E}_i')\vec{x}_j \\
& \hspace{2em} +\vec{x}_j'(\vec{d}_i-\frac{1}{2}\mathbf{E}_i
(\mathbf{A}_i^*)^{-1}(\vec{b}_i+\sum_k\vec{m}_k)) \\
& \hspace{2em} -\frac{1}{4}(\vec{b}_i+\sum_k\vec{m}_k)'(\mathbf{A}_i^*)^{-1}
(\vec{b}_i+\sum_k\vec{m}_k) +\sum_k r_k + f_i \\
& \hspace{2em} +\frac{n_i}{2}\log 2\pi - \frac{1}{2}\log|-2\mathbf{A}_i^*|)
\end{split}\end{equation}$$

</font>

## Mitov et al. 2020: Solve recursion (IV)

<font size="4">

$$\begin{equation}\begin{split}
\delta_{i\rightarrow j} &= p(\mathbf{X}_i\mid\vec{x}_j) \\
&= \mathcal{C}(\mathbf{X}_i,\vec{x}_j;\mathbf{K}_i,\vec{h}_i,g_i) \\
&= \exp(\vec{x}_j\cdot\mathbf{L}_i\vec{x}_j+\vec{x}_j\cdot\vec{m}_i+r_i)
\end{split}\end{equation}$$

$\mathbf{L}_i=\mathbf{C}_i-\frac{1}{4}\mathbf{E}_i(\mathbf{A}_i+\sum_k
\mathbf{L}_k)^{-1}\mathbf{E}_i'$

$\vec{m}_i=\vec{d}_i-\frac{1}{2}\mathbf{E}_i(\mathbf{A}_i+\sum_k
\mathbf{L}_k)^{-1}(\vec{b}_i+\sum_k\vec{m}_k)$

$\begin{equation}\begin{split}
r_i &= -\frac{1}{4}(\vec{b}_i+\sum_k\vec{m}_k)'(\mathbf{A}_i+
\sum_k\mathbf{L}_k)^{-1}(\vec{b}_i+\sum_k\vec{m}_k) +\sum_k r_k + f_i \\
& \hspace{1em} +\frac{n_i}{2}\log 2\pi - \frac{1}{2}\log|-
2(\mathbf{A}_i+\sum_k\mathbf{L}_k)|
\end{split}\end{equation}$

These recursive relations, which <hl>**assume that** $j=\text{Pa}_i$</hl>, are
consistent with those given in Mitov et al. 2020, except that $\mathbf{L}$,
$\vec{m}$ and $r$ are defined slightly differently.

</font>

## Mitov et al. 2020: Root belief

<font size="4">

If node $i$ is a tip, then $\delta_{i\rightarrow j}=
p(\mathbf{X}_i\mid\vec{x}_j)=p(\vec{x}_i\mid\vec{x}_j)$.

The product of incoming messages, $\prod_{i\in\text{Ch}_j}
\delta_{i\rightarrow j}=p(\mathbf{X}_j\mid\vec{x}_j)$.

Recall that we described each clique/cluster as corresponding to a CPD. There is
one caveat: we also include a clique that corresponds to the prior for the root
node, $p(\vec{x}_r)$. This is our root clique.

The belief (i.e. final potential) for the root clique is
$$\begin{equation}\begin{split}
\beta_r &= \phi_r\prod_{k\in\text{Ch}_r}\delta_{k\rightarrow r} \\
&= p(\vec{x}_r)\prod_{k\in\text{Ch}_r}p(\mathbf{X}_k\mid\vec{x}_r) \\
&= p(\mathbf{X},\vec{x}_r)
\end{split}\end{equation}$$

We can set any clique corresponding to an internal node, say $x_i$, as the root clique. However, we would need to derive formulas for messages flowing from the
original root clique towards the new root clique.

The belief for the new root clique is $\beta_i=p(\mathbf{X},\vec{x}_i)$.

</font>

## Mitov et al. 2020: Rerooting

<font size="4">

Previously, we introduced a clique that corresponds specifically to 
$p(\vec{x}_r)$. Instead, we could have assigned $p(\vec{x}_r)$ to any of the
cliques that corresponds to a CPD with $\vec{x}_r$ as the conditioning variable.

Example: Suppose that root node $r$ has 2 children: nodes $r_1$ and $r_2$. Set
the clique corresponding to $p(\vec{x}_{r_1}\mid\vec{x}_r)$ as the root clique
and assign $p(\vec{x}_r)$ to it as well, so that its initial potential is
$\phi_{r_1}=p(\vec{x}_r)p(\vec{x}_{r_1}\mid\vec{x}_r)=p(\vec{x}_{r_1})
p(\vec{x}_r\mid\vec{x}_{r_1})$. The following must hold

$\begin{equation}\begin{split}
\beta_{r_1} &= \phi_{r_1}\prod_{k\in\text{Nb}_{r_1}}\delta_{k\rightarrow r_1}
= p(\mathbf{X},\vec{x}_{r_1},\vec{x}_r) \\ 
p(\mathbf{X}_{r_1}\mid\vec{x}_{r_1})p(\mathbf{X}_{r_2},\vec{x}_r\mid
\vec{x}_{r_1})p(\vec{x}_{r_1}) &= \phi_{r_1}\delta_{r\rightarrow r_1}
\prod_{k\in\text{Ch}_{r_1}}\delta_{k\rightarrow r_1} \\
p(\mathbf{X}_{r_2}\mid\vec{x}_r,\vec{x}_{r_1})p(\vec{x}_r\mid\vec{x}_{r_1}) &=
p(\vec{x}_r\mid\vec{x}_{r_1})\delta_{r\rightarrow r_1} \\
\delta_{r\rightarrow r_1} &= p(\mathbf{X}_{r_2}\mid\vec{x}_{r})
\end{split}\end{equation}$

Generally,
$$\begin{equation}
\delta_{i\rightarrow j} = 
\begin{cases}
p(\mathbf{X}_i\mid\vec{x}_j), \ j=\text{Pa}_i \\
p(\mathbf{X}_{-j}\mid\vec{x}_i), \ i=\text{Pa}_j
\end{cases}
\end{equation}$$
where $\mathbf{X}_{-i}=\mathbf{X}\setminus\mathbf{X}_i$.

</font>

## Mitov et al. 2020: Edge reversal (I)

<font size="4">

If the clique tree is rerooted at a clique that corresponds to CPD
$p(\vec{x}_i\mid\vec{x}_{\text{Pa}_i})$, where node $i$ is internal, then
messages of the type $\delta_{\text{Pa}_i\rightarrow i}$ also need to be
generated.

- $\text{Scope}[\delta_{i\rightarrow\text{Pa}_i}]=\vec{x}_{\text{Pa}_i}=
\text{Scope}[\delta_{\text{Pa}_i\rightarrow i}]$
- Only messages/edges along the (directed) path from node $r$ to node $i$ need
to be reversed!

Suppose node $R$ has $k$ children indexed by $R_1,\ldots,R_k$, and node $i$ is a
descendant of $R_1$. Index the child of node $R_1$ that is along the path to
node $i$ by $l$.

$\begin{equation}\begin{split}
\delta_{R_1\rightarrow l} &= \int\phi_{R_1}
\prod_{j=2}^k\delta_{R_j\rightarrow R_1}d\vec{x}_R \\
&= \int\phi_{R_1}\prod_{j=2}^k \exp(\vec{x}_R\cdot\mathbf{L}_{R_j}\vec{x}_R
+\vec{x}_R\cdot\vec{m}_{R_j}+r_{R_j})d\vec{x}_R \\
&= \int\phi_{R_1}\exp(\vec{x}_{R}'(\sum_{j\ge 2}\mathbf{L}_{R_j})\vec{x}_{R}+
\vec{x}_{R}'(\sum_{j\ge 2}\vec{m}_{R_j})+\sum_{j\ge 2} r_{R_j})d\vec{x}_R
\end{split}\end{equation}$

</font>

## Mitov et al. 2020: Edge reversal (II)

<font size="4">

Assuming $\vec{x}_R\sim\mathcal{N}$ (i.e. Gaussian prior)

$\begin{equation}\begin{split}
\phi_{R_1} &= p(\vec{x}_{R})p(\vec{x}_{R_1}\mid\vec{x}_R) \\
&= \exp(\vec{x}_{R_1}\cdot\mathbf{A}_{R_1}\vec{x}_{R_1}+
\vec{x}_{R_1}\cdot\vec{b}_{R_1} \\
& \hspace{2em} +\vec{x}_R\cdot(\mathbf{C}_R+\mathbf{C}_{R_1})\vec{x}_R+
\vec{x}_R\cdot(\vec{d}_R+\vec{d}_{R_1}) \\
& \hspace{2em} +\vec{x}_R\cdot\mathbf{E}_{R_1}\vec{x}_{R_1}+f_R+f_{R_1})
\end{split}\end{equation}$

$\begin{equation}\begin{split}
\delta_{R_1\rightarrow l} &= \prod_{i\in\text{Ch}_{R_1}\setminus\{l\}}
\delta_{i\rightarrow R_1}\int\phi_{R_1}\exp(\vec{x}_{R}'
\sum_{j\ge 2}\mathbf{L}_{R_j}\vec{x}_{R}+\vec{x}_{R}'
\sum_{j\ge 2}\vec{m}_{R_j}+\sum_{j\ge 2} r_{R_j})d\vec{x}_R \\
&= \exp(\vec{x}_{R_1}'\sum_i\mathbf{L}_i\vec{x}_{R_1}+\vec{x}_{R_1}'
\sum_i\vec{m}_i+\sum_i r_i)\cdot \\
& \hspace{1em}\exp(\vec{x}_{R_1}'\mathbf{L}_{R_1}\vec{x}_{R_1}+\vec{x}'_{R_1}\vec{m}_{R_1}
+r_{R_1})
\end{split}\end{equation}$

</font>

## Mitov et al. 2020: Edge reversal (III)

<font size="4">

$\delta_{R_1\rightarrow l}=\exp(\vec{x}_{R_1}'(\mathbf{L}_{R_1}+
\sum_i\mathbf{L}_i)\vec{x}_{R_1}+\vec{x}_{R_1}'(\vec{m}_{R_1}+\sum_i\vec{m}_i)
+r_{R_1}+\sum_i r_i)$

where from the results of **Solve recursion (I)-(IV)** we have

$\mathbf{L}_{R_1}=\mathbf{A}_{R_1}-\frac{1}{4}\mathbf{E}_{R_1}'(\mathbf{C}_R+
\mathbf{C}_{R_1}+\sum_{j\ge 2}\mathbf{L}_{R_j})^{-1}\mathbf{E}_{R_1}$

$\vec{m}_{R_1} = \vec{b}_{R_1}-\frac{1}{2}\mathbf{E}_{R_1}'
(\mathbf{C}_R+\mathbf{C}_{R_1}+\sum_{j\ge 2}\mathbf{L}_{R_j})^{-1}
(\vec{d}_R+\vec{d}_{R_1}+\sum_{j\ge 2}\vec{m}_{R_j})$

$\begin{equation}\begin{split}
r_{R_1} &= -\frac{1}{4}(\vec{d}_R+\vec{d}_{R_1}+\sum_{j\ge 2}\vec{m}_{R_j})'
(\mathbf{C}_R+\mathbf{C}_{R_1}+\sum_{j\ge 2}\mathbf{L}_{R_j})^{-1}
(\vec{d}_R+\vec{d}_{R_1}+\sum_{j\ge 2}\vec{m}_{R_j}) \\
& +\sum_{j\ge 2}r_{R_j}+f_R+f_{R_1}+\frac{n_R}{2}\log 2\pi
-\frac{1}{2}\log|-2(\mathbf{C}_R+\mathbf{C}_{R_1}+\sum_{j\ge 2}
\mathbf{L}_{R_j})|
\end{split}\end{equation}$

These recursive relations hold for $\delta_{i\rightarrow l}$ when 
$i=\text{Pa}_l$.

Generally, if $i\notin\text{Ch}_R$, then the $\sum_{j\ge 2}
\mathbf{L}_{R_j}$ and $\sum_{j\ge 2}\vec{m}_{R_j}$ terms can be replaced with
$\mathbf{L}_{\text{Pa}_i}$ and $\vec{m}_{\text{Pa}_i}$ respectively.

</font>